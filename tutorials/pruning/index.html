<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/bitstream-hackathon/libs/highlight/github.min.css"> <link rel=stylesheet  href="/bitstream-hackathon/css/jtd.css"> <link rel=icon  href="/bitstream-hackathon/assets/favicon.ico"> <title>Pruning Tutorial</title> <div class=page-wrap > <div class=side-bar > <!-- <div class=header > <a href="/bitstream-hackathon/" class=title > <img src="/bitstream-hackathon/assets/pharm_homepage.jpg"> </a> </div> --> <label for=show-menu  class=show-menu >MENU</label> <input type=checkbox  id=show-menu  role=button > <div class=menu  id=side-menu > <ul class=menu-list > <li class="menu-list-item "><a href="/bitstream-hackathon/" class="menu-list-link ">Home</a> <li class=menu-list-item >Tutorials <ul class=menu-list-child-list  style="display: block;"> <li class="menu-list-item "><a href="/bitstream-hackathon/tutorials/overview/" class="menu-list-link ">Overview</a> <li class="menu-list-item "><a href="/bitstream-hackathon/tutorials/bitstream/" class="menu-list-link ">Bitstreams 101</a> <li class="menu-list-item "><a href="/bitstream-hackathon/tutorials/bitstreamlining/" class="menu-list-link ">Bitstreamlining</a> <li class="menu-list-item active"><a href="/bitstream-hackathon/tutorials/pruning/" class="menu-list-link active">Pruning</a> <li class="menu-list-item "><a href="/bitstream-hackathon/tutorials/mobilenet/" class="menu-list-link ">Simulating MobileNet</a> <li class="menu-list-item "><a href="/bitstream-hackathon/tutorials/submission/" class="menu-list-link ">Submission guide</a> </ul> </ul> </div> <div class=footer > This is <em>Just the docs</em>, adapted from the <a href="https://github.com/pmarsceill/just-the-docs" target=_blank >Jekyll theme</a>. </div> </div> <div class=main-content-wrap > <div class=main-content > <div class=main-header > <span style="padding-right: 250px;"> <img src="/bitstream-hackathon/assets/red-flush-UWlogo.jpg" width=150px > <img src="/bitstream-hackathon/assets/pharm_homepage.jpg" width=175px  style="padding-bottom: 10px;"> </span> <a id=github  href="/bitstream-hackathon//github.com/UW-PHARM/BitSAD.jl">BitSAD.jl on GitHub</a> </div> <div class=franklin-content ><em>Make sure you have completed the <a href="/bitstream-hackathon/tutorials/overview">getting started</a> tutorial.</em></p> <p><strong>Table of contents:</strong></p> <p><div class=franklin-toc ><ol><li><a href="#introduction">Introduction</a><li><a href="#weight_pruning">Weight Pruning</a><ol><li><a href="#unstructured_pruning">Unstructured Pruning</a><li><a href="#structured_pruning">Structured Pruning</a><li><a href="#propagating_the_pruning">Propagating the pruning</a><li><a href="#resizing_the_propagated_model">Resizing the propagated model</a><li><a href="#pruning_and_finetuning_pipeline">Pruning and Finetuning pipeline</a></ol></ol></div> <h1 id=pruning_tutorial ><a href="#pruning_tutorial" class=header-anchor >Pruning tutorial</a></h1> <h2 id=introduction ><a href="#introduction" class=header-anchor >Introduction</a></h2> <p>Deep learning has achieved unprecedented performance on image recognition tasks like ImageNet and natural language processing tasks such as question answering and machine translation. These models generally are on the order of millions or even billions of parameters. For example, Google&#39;s recent released large language model, <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">Pathways Language Model &#40;PaLM&#41;</a>, can do well on tasks like conceptual understanding and cause &amp; effect reasoning and contains over 540 Billion parameters&#33;</p> <p>While this is great for advancing state of the art &#40;SOTA&#41; in terms of accuracy, we have applications like self-driving cars or client-side video processing where we would want to deploy these models on the edge to meet real-time deadlines. By having models on the device, we can avoid the latency cost of sending a request to the server for the model to process and sending back the output. In these settings, we can&#39;t use these gigantic models for a few reasons: the on-device memory available is quite limited &#40;meaning we can&#39;t fit our model into memory&#41; and the number of operations it takes to obtain output from the model would fail constraints like latency and power. Luckily, we can rely on model compression to address these concerns.</p> <p>Model compression is the area of research focused on deploying SOTA models in resource-constrained devices while minimizing accuracy degradation. Various approaches to compressing a model include: weight pruning, quantization, knowledge distillation, low-rank tensor decomposition, hardware-aware neural architecture search, etc. In particular, we will discuss and target, arguably, the simplest of these methods: weight pruning.</p> <h2 id=weight_pruning ><a href="#weight_pruning" class=header-anchor >Weight Pruning</a></h2> <h3 id=unstructured_pruning ><a href="#unstructured_pruning" class=header-anchor >Unstructured Pruning</a></h3> <p>Pruning a deep learning models involves finding a percentage of the weights that don&#39;t contribute much to the classification output and setting their values to 0. By setting the values to 0, we reduce the memory footprint of the model as well as the number of multiplies and accumulates during inference. The de-facto method is low-weight magnitude pruning where we rank the weights in the network and eliminate the smallest weights upto a threshold dictated by a chosen compression ratio. Let&#39;s explore how to do this in Julia with the Flux.jl package.</p> <p>First, let&#39;s begin with some imports that will help us load the dataset and model.</p> <pre><code class="julia hljs">include(<span class=hljs-string >&quot;_tutorials/src/setup.jl&quot;</span>);</code></pre>
<p>Next, let&#39;s define our model. We are using <a href="https://arxiv.org/abs/1704.04861">MobileNetv1</a>, which is a popular deep learning model that achieves high classification accuracies while still being very resource-efficient. Note the number of parameters the model contains and the amount of memory needed to store the model. We can also calculate the number of multiplies and accumulates that MobileNetv1 incurs to produce an output.</p>
<pre><code class="julia hljs">m = MobileNet(slopehtanh, <span class=hljs-number >0.25</span>; fcsize = <span class=hljs-number >64</span>, nclasses = <span class=hljs-number >1</span>)
mults, adds, output_size = compute_dot_prods(m, (<span class=hljs-number >96</span>, <span class=hljs-number >96</span>, <span class=hljs-number >3</span>, <span class=hljs-number >1</span>)) <span class=hljs-comment ># height and weight are 96, input channels are 3, batch size = 1</span>
println(<span class=hljs-string >&quot;MobileNet Mults &quot;</span>, mults, <span class=hljs-string >&quot; Adds &quot;</span>, adds)</code></pre><pre><code class="plaintext hljs">MobileNet Mults 7505600 Adds 7273983
</code></pre>
<p>Next, we need to load in the dataset to prune and finetune our model. show line that loads in the data. Now that we&#39;ve finished our setup, let&#39;s prune our model. We can use the <code>FluxPrune.jl</code> package to easily prune the lowest magnitude weights by calling <code>LevelPrune</code>.</p>
<pre><code class="julia hljs">m_lv_pruned = prune(LevelPrune(<span class=hljs-number >0.2</span>), m);</code></pre>
<p><code>FluxPrune</code>&#39;s prune function takes in two inputs: the pruning strategy and the model to prune. We are using the <code>LevelPrune</code> strategy which traverses each layer of the model and removes the lowest <code>p&#37;</code> &#40;<code>10&#37;</code> in this case&#41; weights in each layer. This is called unstructured pruning since we are concerned with removing the lowest magnitude weights and not worrying about if the sparsity induces some kind of structure. <code>FluxPrune</code> allows you to set a different pruning strategy for every layer in the model if you desire. Typically, we also have to finetune our resulting pruned model in order to recover some accuracy penalty induced by setting the weights to 0. Let&#39;s compute the number of multiplies and accumulates to see how much we have saved.</p>
<pre><code class="julia hljs">mults, adds, output_size = compute_dot_prods(m_lv_pruned, (<span class=hljs-number >96</span>, <span class=hljs-number >96</span>, <span class=hljs-number >3</span>, <span class=hljs-number >1</span>)) <span class=hljs-comment ># height and weight are 96, input channels are 3, batch size = 1</span>
println(<span class=hljs-string >&quot;MobileNet Mults &quot;</span>, mults, <span class=hljs-string >&quot; Adds &quot;</span>, adds)</code></pre><pre><code class="plaintext hljs">MobileNet Mults 7460806 Adds 7273983
</code></pre>
<p>We can see that we have obtained a reduction in the number of multiplies relative to our unpruned baseline. Unstructured pruning is powerful in that we are able to prune so aggressively that we can obtain sparse models that perform just as well as the baseline at less than <code>10&#37;</code> of the original model capacity. While unstructured pruning achieves the best compression vs. accuracy tradeoffs, it may not translate into faster inference since the unstructured nature of zeros in the weight matrices may induce irregular memory access patterns and sparse GEMM kernels are competitive with dense ones only at extreme sparsities. For these reasons, one may consider structured pruning instead.</p>
<h3 id=structured_pruning ><a href="#structured_pruning" class=header-anchor >Structured Pruning</a></h3>
<p>In structured pruning, we remove entire channels &#40;typically&#41; or filters rather than individual weights. This type of pruning only applies to the convolutional layers, as the concept of removing structure really applies to conv layers as opposed to full-connected layers. By removing the lowest magnitude channels, we are drastically able to reduce the number of multiplies and accumulates that our model has to perform.</p>
<p>To prune channels, we can define the <code>ChannelPrune</code> strategy, which solely targets the convolutional layers.</p>
<pre><code class="julia hljs">m_ch_pruned = prune(ChannelPrune(<span class=hljs-number >0.2</span>), m);
mults, adds, output_size = compute_dot_prods(m_ch_pruned, (<span class=hljs-number >96</span>, <span class=hljs-number >96</span>, <span class=hljs-number >3</span>, <span class=hljs-number >1</span>)) <span class=hljs-comment ># height and weight are 96, input channels are 3, batch size = 1</span>
println(<span class=hljs-string >&quot;MobileNet Mults &quot;</span>, mults, <span class=hljs-string >&quot; Adds &quot;</span>, adds)</code></pre><pre><code class="plaintext hljs">MobileNet Mults 6105200 Adds 5915181
</code></pre>
<p>Compared to the number of multiplies reduced from unstructured pruning, structured pruning drastically reduces the computational cost incurred by the model during inference. The caveat for structured pruning is that by eliminating groups of weights, the compression ratio that structured pruning methods are set at are much lower than those from unstructured methods so the memory savings are limited. Choosing what the optimal amount of compression vs. latency of the model during inference is a design choice that must be made during model design and prior to deployment.</p>
<h3 id=propagating_the_pruning ><a href="#propagating_the_pruning" class=header-anchor >Propagating the pruning</a></h3>
<p>Once we have a sparse model through structured or unstructured pruning, there may be convolution nodes that have been completely eliminated in the pruning process. During the propagating phase, we evaluate which other neighbouring nodes have been impacted by the effects of pruning, and zero them out as well to extract further sparsity from pur pruning without impacting accuracy.</p>
<pre><code class="julia hljs">m_pruned = keepprune(m_ch_pruned)
m_prop = prune_propagate(m_pruned)
mults, adds, output_size = compute_dot_prods(m_prop, (<span class=hljs-number >96</span>, <span class=hljs-number >96</span>, <span class=hljs-number >3</span>, <span class=hljs-number >1</span>))
println(<span class=hljs-string >&quot;Propagated MobileNet Mults &quot;</span>, mults, <span class=hljs-string >&quot; Adds &quot;</span>, adds)</code></pre><pre><code class="plaintext hljs">Propagated MobileNet Mults 4991603 Adds 4887228
</code></pre>
<h3 id=resizing_the_propagated_model ><a href="#resizing_the_propagated_model" class=header-anchor >Resizing the propagated model</a></h3>
<p>If enough nodes get pruned out, there would be slices in the model which accomplish nothing, computationally. Instead of wasting resources on passing these kernels full of zeros around, they can be eliminated from the structure of our model.</p>
<pre><code class="julia hljs">m_resized = resize(m_prop)
mults, adds, output_size = compute_dot_prods(m_resized, (<span class=hljs-number >96</span>, <span class=hljs-number >96</span>, <span class=hljs-number >3</span>, <span class=hljs-number >1</span>))
println(<span class=hljs-string >&quot;Resized MobileNet Mults &quot;</span>, mults, <span class=hljs-string >&quot; Adds &quot;</span>, adds)</code></pre><pre><code class="plaintext hljs">Resized MobileNet Mults 3682228 Adds 3529002
</code></pre>
<h3 id=pruning_and_finetuning_pipeline ><a href="#pruning_and_finetuning_pipeline" class=header-anchor >Pruning and Finetuning pipeline</a></h3>
<p>Now that we seen how to prune our model, let&#39;s try to finetune it to recover some of the accuracy we lost. A basic template setup for training the model is provided by trainer function, and can be used as a starting point for your own training methodology.</p>
<pre><code class="julia hljs">include(<span class=hljs-string >&quot;_tutorials/trainerfunc.jl&quot;</span>);
trainer(m_resized, <span class=hljs-number >1</span>) <span class=hljs-comment >#trains resized model for 2 epochs</span></code></pre><pre><code class="plaintext hljs">┌─────────────────────────────────────┬───────┬─────────┬───────────┐
│                               Phase │ Epoch │    Loss │ train_acc │
├─────────────────────────────────────┼───────┼─────────┼───────────┤
│ FluxTraining.Phases.TrainingPhase() │   1.0 │ 0.70822 │   0.53334 │
└─────────────────────────────────────┴───────┴─────────┴───────────┘
┌───────────────────────────────────────┬───────┬─────────┬─────────┐
│                                 Phase │ Epoch │ val_acc │    Loss │
├───────────────────────────────────────┼───────┼─────────┼─────────┤
│ FluxTraining.Phases.ValidationPhase() │   1.0 │  0.5307 │ 0.69801 │
└───────────────────────────────────────┴───────┴─────────┴─────────┘
Chain(
  Chain(
    Conv((3, 3), 3 =&gt; 8, pad=1, stride=2, bias=false),  # 216 parameters
    BatchNorm(8, slopehtanh),           # 16 parameters, plus 16
    Conv((3, 3), 8 =&gt; 8, pad=1, groups=8, bias=false),  # 72 parameters
    BatchNorm(8, slopehtanh),           # 16 parameters, plus 16
    Conv((1, 1), 8 =&gt; 11),              # 99 parameters
    BatchNorm(11, slopehtanh),          # 22 parameters, plus 22
    Conv((3, 3), 11 =&gt; 11, pad=1, stride=2, groups=11, bias=false),  # 99 parameters
    BatchNorm(11, slopehtanh),          # 22 parameters, plus 22
    Conv((1, 1), 11 =&gt; 21),             # 252 parameters
    BatchNorm(21, slopehtanh),          # 42 parameters, plus 42
    Conv((3, 3), 21 =&gt; 21, pad=1, groups=21, bias=false),  # 189 parameters
    BatchNorm(21, slopehtanh),          # 42 parameters, plus 42
    Conv((1, 1), 21 =&gt; 20),             # 440 parameters
    BatchNorm(20, slopehtanh),          # 40 parameters, plus 40
    Conv((3, 3), 20 =&gt; 20, pad=1, stride=2, groups=20, bias=false),  # 180 parameters
    BatchNorm(20, slopehtanh),          # 40 parameters, plus 40
    Conv((1, 1), 20 =&gt; 44),             # 924 parameters
    BatchNorm(44, slopehtanh),          # 88 parameters, plus 88
    Conv((3, 3), 44 =&gt; 44, pad=1, groups=44, bias=false),  # 396 parameters
    BatchNorm(44, slopehtanh),          # 88 parameters, plus 88
    Conv((1, 1), 44 =&gt; 41),             # 1_845 parameters
    BatchNorm(41, slopehtanh),          # 82 parameters, plus 82
    Conv((3, 3), 41 =&gt; 41, pad=1, stride=2, groups=41, bias=false),  # 369 parameters
    BatchNorm(41, slopehtanh),          # 82 parameters, plus 82
    Conv((1, 1), 41 =&gt; 83),             # 3_486 parameters
    BatchNorm(83, slopehtanh),          # 166 parameters, plus 166
    Conv((3, 3), 83 =&gt; 83, pad=1, groups=83, bias=false),  # 747 parameters
    BatchNorm(83, slopehtanh),          # 166 parameters, plus 166
    Conv((1, 1), 83 =&gt; 85),             # 7_140 parameters
    BatchNorm(85, slopehtanh),          # 170 parameters, plus 170
    Conv((3, 3), 85 =&gt; 85, pad=1, groups=85, bias=false),  # 765 parameters
    BatchNorm(85, slopehtanh),          # 170 parameters, plus 170
    Conv((1, 1), 85 =&gt; 86),             # 7_396 parameters
    BatchNorm(86, slopehtanh),          # 172 parameters, plus 172
    Conv((3, 3), 86 =&gt; 86, pad=1, groups=86, bias=false),  # 774 parameters
    BatchNorm(86, slopehtanh),          # 172 parameters, plus 172
    Conv((1, 1), 86 =&gt; 82),             # 7_134 parameters
    BatchNorm(82, slopehtanh),          # 164 parameters, plus 164
    Conv((3, 3), 82 =&gt; 82, pad=1, groups=82, bias=false),  # 738 parameters
    BatchNorm(82, slopehtanh),          # 164 parameters, plus 164
    Conv((1, 1), 82 =&gt; 81),             # 6_723 parameters
    BatchNorm(81, slopehtanh),          # 162 parameters, plus 162
    Conv((3, 3), 81 =&gt; 81, pad=1, groups=81, bias=false),  # 729 parameters
    BatchNorm(81, slopehtanh),          # 162 parameters, plus 162
    Conv((1, 1), 81 =&gt; 82),             # 6_724 parameters
    BatchNorm(82, slopehtanh),          # 164 parameters, plus 164
    Conv((3, 3), 82 =&gt; 82, pad=1, stride=2, groups=82, bias=false),  # 738 parameters
    BatchNorm(82, slopehtanh),          # 164 parameters, plus 164
    Conv((1, 1), 82 =&gt; 162),            # 13_446 parameters
    BatchNorm(162, slopehtanh),         # 324 parameters, plus 324
    Conv((3, 3), 162 =&gt; 162, pad=1, groups=162, bias=false),  # 1_458 parameters
    BatchNorm(162, slopehtanh),         # 324 parameters, plus 324
    Conv((1, 1), 162 =&gt; 256),           # 41_728 parameters
    BatchNorm(256, slopehtanh),         # 512 parameters, plus 512
  ),
  Chain(
    GlobalMeanPool(),
    MLUtils.flatten,
    Dense(256 =&gt; 64, slopehtanh),       # 16_448 parameters
    Dense(64 =&gt; 1),                     # 65 parameters
  ),
)         # Total: 98 trainable arrays, 125_056 parameters,
          # plus 54 non-trainable, 3_736 parameters, summarysize 523.531 KiB.</code></pre>
<p>Useful Resources:</p>
<ol>
<li><p><a href="https://intellabs.github.io/distiller/pruning.html">Blog Post on Pruning and Sparsity</a></p>

<li><p><a href="https://medium.com/gsi-technology/an-overview-of-model-compression-techniques-for-deep-learning-in-space-3fd8d4ce84e5">Blog Post on Model Compression</a></p>

<li><p><a href="https://arxiv.org/abs/1710.0928">Model Compression Survey Paper</a></p>

<li><p><a href="https://arxiv.org/abs/1510.00149">Deep Compression Paper</a></p>

</ol>


<div class=page-foot >
    <hr>
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> UW-Madison PHARM Group. Last modified: October 22, 2023.
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
</div>
</div>
    </div> 
    </div> 
    </div> <!-- end of class page-wrap-->