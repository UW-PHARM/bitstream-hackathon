<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/bitstream-hackathon/libs/highlight/github.min.css"> <link rel=stylesheet  href="/bitstream-hackathon/css/jtd.css"> <link rel=icon  href="/bitstream-hackathon/assets/favicon.ico"> <title>Bitstreamlining</title> <div class=page-wrap > <div class=side-bar > <!-- <div class=header > <a href="/bitstream-hackathon/" class=title > <img src="/bitstream-hackathon/assets/pharm_homepage.jpg"> </a> </div> --> <label for=show-menu  class=show-menu >MENU</label> <input type=checkbox  id=show-menu  role=button > <div class=menu  id=side-menu > <ul class=menu-list > <li class="menu-list-item "><a href="/bitstream-hackathon/" class="menu-list-link ">Home</a> <li class=menu-list-item >Tutorials <ul class=menu-list-child-list  style="display: block;"> <li class="menu-list-item "><a href="/bitstream-hackathon/tutorials/overview/" class="menu-list-link ">Overview</a> <li class="menu-list-item "><a href="/bitstream-hackathon/tutorials/bitstream/" class="menu-list-link ">Bitstreams 101</a> <li class="menu-list-item active"><a href="/bitstream-hackathon/tutorials/bitstreamlining/" class="menu-list-link active">Bitstreamlining</a> <li class="menu-list-item "><a href="/bitstream-hackathon/tutorials/pruning/" class="menu-list-link ">Pruning</a> <li class="menu-list-item "><a href="/bitstream-hackathon/tutorials/mobilenet/" class="menu-list-link ">Simulating MobileNet</a> <li class="menu-list-item "><a href="/bitstream-hackathon/tutorials/submission/" class="menu-list-link ">Submission guide</a> </ul> </ul> </div> <div class=footer > This is <em>Just the docs</em>, adapted from the <a href="https://github.com/pmarsceill/just-the-docs" target=_blank >Jekyll theme</a>. </div> </div> <div class=main-content-wrap > <div class=main-content > <div class=main-header > <span style="padding-right: 250px;"> <img src="/bitstream-hackathon/assets/red-flush-UWlogo.jpg" width=150px > <img src="/bitstream-hackathon/assets/pharm_homepage.jpg" width=175px  style="padding-bottom: 10px;"> </span> <a id=github  href="/bitstream-hackathon//github.com/UW-PHARM/BitSAD.jl">BitSAD.jl on GitHub</a> </div> <div class=franklin-content ><em>Make sure you have completed the <a href="/bitstream-hackathon/tutorials/overview">getting started</a> tutorial.</em></p> <p><strong>Table of contents:</strong></p> <p><div class=franklin-toc ><ol><li><a href="#activation_function">Activation function</a><li><a href="#penalizing_saturation_of_parameters">Penalizing saturation of parameters</a><li><a href="#training_without_the_batchnorm">Training without the Batchnorm</a></ol></div> <h1 id=bitstreamlining_training_a_model_for_bitstream_computing ><a href="#bitstreamlining_training_a_model_for_bitstream_computing" class=header-anchor >Bitstreamlining: Training a model for bitstream computing</a></h1> <p>The biggest caveat with bitstream is that all our values need to be restricted within &#91;-1, 1&#93;. We will now go over some of the changes that help a floating point training system better align with bitstream constraints.</p> <h2 id=activation_function ><a href="#activation_function" class=header-anchor >Activation function</a></h2> <p>A better fit for our range constraint is to use <a href="https://fluxml.ai/Flux.jl/stable/models/activation/#NNlib.hardtanh">hardtanh</a> as the activation function. Apart from being much cheaper computationally, it would mimic the behavior of not having any explicit activation function in the bitstream model.</p> <p>However, you might notice this is not the activation function present in the pretrained model. Due to the saturating nature of hardtanh, no &quot;learning&quot; might occur once values saturate to a magnitude of 1. Therefore, we add a slight <a href="https://arxiv.org/abs/1603.00391">negative slope</a> to hardtanh beyond our range of interest in the function slopehtanh&#40;&#41; for training the model.</p> <h2 id=penalizing_saturation_of_parameters ><a href="#penalizing_saturation_of_parameters" class=header-anchor >Penalizing saturation of parameters</a></h2> <p>When the floating point model is ported to the bitstream realm, any parameters larger than 1 in magnitude would saturate their stream. What that means for our accuracy is, that the parameter has now changed its value and accuracy might decrease more than anticipated if too many parameters get saturated during training. One way to tackle this problem is to penalize saturated parameters during training. We can do this by adding a <a href="https://fluxml.ai/Flux.jl/stable/models/activation/#NNlib.softshrink">softshrink</a> &#40;with Î»&#61;1&#41; of all parameters to the loss in training phase, creating a magnitude-aware training scheme.</p> <p>The functions enable&#95;shrinkloss and disable&#95;shrinkloss can help toggle this functionality on and off. It is highly recommended to have enable_shrinkloss&#40;1&#41; before you train your model. It is set to 1 by default.</p> <h2 id=training_without_the_batchnorm ><a href="#training_without_the_batchnorm" class=header-anchor >Training without the Batchnorm</a></h2> <p>While the BatchNorm helps our model train, it can be a place for parameter saturations to hide. Once we are satisfied with the way our model is trained, we can merge the batchnorms and remove these saturations, but that can slightly decrease the accuracy. So, we merge the model for a few epochs to recoup the lost accuracy. Do note that if the dip in accuracy from validation phase to merging of batchnorms is high, it might be better to splice in blank batch norm layers and train the model as training can become more erratic without the batchnorm layers. On merging batchnorm, it would be helpful to desaturate the model once so that any saturations just get replaced by -1 or 1s respectively and do not make the parameter based loss explode.</p> <pre><code class="julia hljs">include(<span class=hljs-string >&quot;./src/setup.jl&quot;</span>);

<span class=hljs-comment ># this pretrained model has good accuracy on evaluating, but needs batchnorms if being trained.</span>
BSON.<span class=hljs-meta >@load</span> <span class=hljs-string >&quot;src\\pretrained.bson&quot;</span> m
m_bn = rebn(m);
<span class=hljs-comment ># m_bn now has blank batchnorms, ready for pruning and training.</span>

<span class=hljs-comment ># this pretrained model still has its batchnorm layers present, which can cause saturations.</span>
BSON.<span class=hljs-meta >@load</span> <span class=hljs-string >&quot;src\\pretrained_BN.bson&quot;</span> m
m_merged = merge_conv_bn(m);
<span class=hljs-comment >#on merging the batchnorm, desaturation is necessary.</span>
m_merged = desaturate(m_merged);
<span class=hljs-comment ># m_merged can now be trained for finetuning.</span></code></pre> <div class=page-foot > <hr> <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> UW-Madison PHARM Group. Last modified: November 11, 2023. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div> <!-- end of class page-wrap-->